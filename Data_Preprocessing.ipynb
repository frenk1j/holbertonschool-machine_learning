{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e5bd41-e2f0-46db-80d7-aab7639c0304",
   "metadata": {},
   "source": [
    "# Data Processing Plan - Portfolio Project\n",
    "\n",
    "Student: Frenki Janaqi  \n",
    "School: Holberton School  \n",
    "Date: November 20, 2025  \n",
    "Project: S&P 500 Preprocessing\n",
    "\n",
    "This notebook documents data sources, formats, features, exploration, hypotheses, missing data/outliers strategy, time-aware splits, bias mitigation, features for training, data types, transformations, and storage plans, aligned to the assignment requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82cc0681-67c2-401e-8144-9ff6322bb484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/mac/Documents/ProvJupyter/all_stocks_5yr.csv/all_stocks_5yr.csv')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Local CSV path\n",
    "CSV_PATH = Path(r\"/Users/mac/Documents/ProvJupyter/all_stocks_5yr.csv\") / \"all_stocks_5yr.csv\"\n",
    "CSV_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead1bef-e8e7-44f2-8c1b-45fafa498bd6",
   "metadata": {},
   "source": [
    "## 1. Data Sources\n",
    "\n",
    "- Primary source: Local CSV located at /Users/mac/Documents/ProvJupyter/all_stocks_5yr.csv.  \n",
    "- Aggregation: Single source for baseline; may later augment with fundamentals or macro time series.  \n",
    "- Access method: pandas.read_csv using a Windows-safe path via pathlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f6875bc-b912-4524-b331-8076ec831b28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-02-08</th>\n",
       "      <td>15.07</td>\n",
       "      <td>15.12</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.75</td>\n",
       "      <td>8407500</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-11</th>\n",
       "      <td>14.89</td>\n",
       "      <td>15.01</td>\n",
       "      <td>14.26</td>\n",
       "      <td>14.46</td>\n",
       "      <td>8882000</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-12</th>\n",
       "      <td>14.45</td>\n",
       "      <td>14.51</td>\n",
       "      <td>14.10</td>\n",
       "      <td>14.27</td>\n",
       "      <td>8126000</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-13</th>\n",
       "      <td>14.30</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.25</td>\n",
       "      <td>14.66</td>\n",
       "      <td>10259500</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-14</th>\n",
       "      <td>14.94</td>\n",
       "      <td>14.96</td>\n",
       "      <td>13.16</td>\n",
       "      <td>13.99</td>\n",
       "      <td>31879900</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             open   high    low  close    volume Name\n",
       "date                                                 \n",
       "2013-02-08  15.07  15.12  14.63  14.75   8407500  AAL\n",
       "2013-02-11  14.89  15.01  14.26  14.46   8882000  AAL\n",
       "2013-02-12  14.45  14.51  14.10  14.27   8126000  AAL\n",
       "2013-02-13  14.30  14.94  14.25  14.66  10259500  AAL\n",
       "2013-02-14  14.94  14.96  13.16  13.99  31879900  AAL"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = Path(\"/Users/mac/Documents/ProvJupyter/all_stocks_5yr.csv\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, nrows=1000, parse_dates=[\"date\"])\n",
    "df = df.sort_values(\"date\").set_index(\"date\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bbac1-cc9a-4c7b-8871-0658845781d2",
   "metadata": {},
   "source": [
    "## 2. Data Format\n",
    "\n",
    "- Current format: CSV with columns typically including date, open, high, low, close, volume, and a ticker identifier (Name).  \n",
    "- Target format: Pandas DataFrame indexed by datetime for time-series operations and Parquet for intermediate storage.  \n",
    "- Structure: Long format with multiple tickers stacked by date, enabling per-ticker rolling features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d93df239-e9ad-4f70-a357-be4f63ca415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['open', 'high', 'low', 'close', 'volume', 'Name']\n",
      "\n",
      "Null fraction (top 20):\n",
      "open      0.0\n",
      "high      0.0\n",
      "low       0.0\n",
      "close     0.0\n",
      "volume    0.0\n",
      "Name      0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nNull fraction (top 20):\")\n",
    "print(df.isna().mean().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd510b01-8c21-4ba9-92c8-abad2e0f3e73",
   "metadata": {},
   "source": [
    "## 3. Features and Exploration\n",
    "\n",
    "- Raw features: open, high, low, close, volume, Name, and date.  \n",
    "- EDA plan: descriptive statistics, missingness summary, distributions, correlations among numeric features, and per-ticker coverage over time.  \n",
    "- Derived features: 1-day/5-day returns, rolling means (volume MA5, close MA5/MA20), and simple momentum indicators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17f68cca-adaf-4945-922e-eee036652add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_model: (999, 13)\n"
     ]
    }
   ],
   "source": [
    "required = {\"open\",\"high\",\"low\",\"close\",\"volume\",\"Name\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "df[\"close_next\"] = df.groupby(\"Name\")[\"close\"].shift(-1)\n",
    "df[\"up_1d\"] = (df[\"close_next\"] > df[\"close\"]).astype(\"Int64\")\n",
    "df[\"ret_1d\"] = df.groupby(\"Name\")[\"close\"].pct_change()\n",
    "df[\"ret_5d\"] = df.groupby(\"Name\")[\"close\"].pct_change(5)\n",
    "df[\"vol_ma5\"] = df.groupby(\"Name\")[\"volume\"].transform(lambda s: s.rolling(5, min_periods=1).mean())\n",
    "df[\"close_ma5\"] = df.groupby(\"Name\")[\"close\"].transform(lambda s: s.rolling(5, min_periods=1).mean())\n",
    "df[\"close_ma20\"] = df.groupby(\"Name\")[\"close\"].transform(lambda s: s.rolling(20, min_periods=1).mean())\n",
    "\n",
    "df_model = df.dropna(subset=[\"close_next\"]).copy()\n",
    "print(\"df_model:\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46dd7f-b300-4fc7-9b95-f34f30f5ba92",
   "metadata": {},
   "source": [
    "## 4. Hypotheses and Testing\n",
    "\n",
    "- Hypothesis 1: Short-term moving averages (e.g., close MA5 vs. MA20) capture momentum informative for next-day direction.  \n",
    "  - Test: correlation/feature importance and evaluation by momentum regimes.  \n",
    "- Hypothesis 2: Recent returns and volume changes have predictive value for next-day up/down.  \n",
    "  - Test: classification metrics and calibration across feature quantiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21fde9b7-0b36-416b-b77b-ba31a2365565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has needed columns: True\n",
      "P(up_1d=1) by momentum regime (0=MA5<=MA20, 1=MA5>MA20):\n",
      "mom_pos\n",
      "0    0.548463\n",
      "1    0.522569\n",
      "Name: up_1d, dtype: Float64\n",
      "\n",
      "P(up_1d=1) by ret_1d quantile bin:\n",
      "ret_1d_bin\n",
      "(-0.0577, -0.0149]        0.465\n",
      "(-0.0149, -0.00312]    0.537688\n",
      "(-0.00312, 0.00698]        0.57\n",
      "(0.00698, 0.0188]      0.557789\n",
      "(0.0188, 0.0593]           0.54\n",
      "Name: up_1d, dtype: Float64\n",
      "\n",
      "Correlation with close_next (preview on first 20k rows):\n",
      "close_next    1.000000\n",
      "close         0.996818\n",
      "high          0.995688\n",
      "low           0.995546\n",
      "open          0.994181\n",
      "close_ma5     0.992252\n",
      "close_ma20    0.978314\n",
      "vol_ma5       0.224725\n",
      "volume        0.146502\n",
      "ret_5d        0.010956\n",
      "ret_1d        0.006913\n",
      "Name: close_next, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/3p8hr7gx18s869lqw7tb61qc0000gn/T/ipykernel_16324/1830105432.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  print(tmp.groupby(\"ret_1d_bin\")[\"up_1d\"].mean())\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sanity: ensure engineered features and targets exist\n",
    "needed_cols = {\"close_ma5\",\"close_ma20\",\"ret_1d\",\"ret_5d\",\"up_1d\",\"close_next\"}\n",
    "print(\"Has needed columns:\", needed_cols.issubset(set(df_model.columns)))\n",
    "\n",
    "# Hypothesis 1: Momentum via moving averages (close_ma5 vs close_ma20)\n",
    "# Create a simple momentum signal and check association with next-day direction (classification target up_1d)\n",
    "if {\"close_ma5\",\"close_ma20\",\"up_1d\"}.issubset(df_model.columns):\n",
    "    df_mom = df_model[[\"close_ma5\",\"close_ma20\",\"up_1d\"]].dropna().copy()\n",
    "    df_mom[\"mom_pos\"] = (df_mom[\"close_ma5\"] > df_mom[\"close_ma20\"]).astype(int)\n",
    "    # Group-wise accuracy proxy: fraction of positive next-day moves when momentum positive vs negative\n",
    "    grp = df_mom.groupby(\"mom_pos\")[\"up_1d\"].mean()\n",
    "    print(\"P(up_1d=1) by momentum regime (0=MA5<=MA20, 1=MA5>MA20):\")\n",
    "    print(grp)\n",
    "\n",
    "# Hypothesis 2: Recent returns and volume/price levels relate to next-day movement\n",
    "# Bin recent return ret_1d into quantiles and compare up_1d rates\n",
    "if {\"ret_1d\",\"up_1d\"}.issubset(df_model.columns):\n",
    "    tmp = df_model[[\"ret_1d\",\"up_1d\"]].dropna().copy()\n",
    "    # Use quantiles; clip extremes to reduce influence of outliers\n",
    "    tmp[\"ret_1d_clip\"] = tmp[\"ret_1d\"].clip(tmp[\"ret_1d\"].quantile(0.01), tmp[\"ret_1d\"].quantile(0.99))\n",
    "    tmp[\"ret_1d_bin\"] = pd.qcut(tmp[\"ret_1d_clip\"], q=5, duplicates=\"drop\")\n",
    "    print(\"\\nP(up_1d=1) by ret_1d quantile bin:\")\n",
    "    print(tmp.groupby(\"ret_1d_bin\")[\"up_1d\"].mean())\n",
    "\n",
    "# Optional: correlation preview for regression target (close_next) with engineered numeric features (sampled)\n",
    "numeric_checks = [\"open\",\"high\",\"low\",\"close\",\"volume\",\"ret_1d\",\"ret_5d\",\"vol_ma5\",\"close_ma5\",\"close_ma20\",\"close_next\"]\n",
    "numeric_checks = [c for c in numeric_checks if c in df_model.columns]\n",
    "if \"close_next\" in numeric_checks:\n",
    "    sample_n = min(len(df_model), 20000)\n",
    "    corr_preview = df_model[numeric_checks].iloc[:sample_n].corr()[\"close_next\"].sort_values(ascending=False)\n",
    "    print(\"\\nCorrelation with close_next (preview on first 20k rows):\")\n",
    "    print(corr_preview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c7b29-c364-42c2-aa54-5aa37d97ebb5",
   "metadata": {},
   "source": [
    "## 5. Data Density, Missing Data, Outliers\n",
    "\n",
    "- Density: OHLCV is generally dense; verify and document any gaps.  \n",
    "- Missing handling: numeric → median imputation; categorical → most frequent in the pipeline for reproducibility.  \n",
    "- Outliers: detect with IQR/Z-score; choose whether to cap or keep due to market shock relevance, documenting the decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4da9a9c-f527-4ce7-8a6d-db7b7b538177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top missing columns:\n",
      "ret_5d        0.005005\n",
      "ret_1d        0.001001\n",
      "open          0.000000\n",
      "high          0.000000\n",
      "low           0.000000\n",
      "close         0.000000\n",
      "volume        0.000000\n",
      "Name          0.000000\n",
      "close_next    0.000000\n",
      "up_1d         0.000000\n",
      "vol_ma5       0.000000\n",
      "close_ma5     0.000000\n",
      "close_ma20    0.000000\n",
      "dtype: float64\n",
      "IQR outlier rates (preview):\n",
      "volume        0.049049\n",
      "vol_ma5       0.042042\n",
      "ret_1d        0.028056\n",
      "ret_5d        0.025151\n",
      "open          0.000000\n",
      "high          0.000000\n",
      "low           0.000000\n",
      "close         0.000000\n",
      "close_ma5     0.000000\n",
      "close_ma20    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Missingness\n",
    "missing = df_model.isna().mean().sort_values(ascending=False)\n",
    "print(\"Top missing columns:\")\n",
    "print(missing.head(15))\n",
    "\n",
    "# Lightweight outlier rate (IQR) on compact numeric set\n",
    "core_num = [\n",
    "    \"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "    \"ret_1d\",\"ret_5d\",\"vol_ma5\",\"close_ma5\",\"close_ma20\"\n",
    "]\n",
    "core_num = [c for c in core_num if c in df_model.columns]\n",
    "\n",
    "def iqr_outlier_rate(s, factor=1.5):\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = q1 - factor*iqr, q3 + factor*iqr\n",
    "    return ((s < lo) | (s > hi)).mean()\n",
    "\n",
    "# Use train later for final rates; for now compute on full to preview\n",
    "preview = pd.Series({c: iqr_outlier_rate(df_model[c].dropna()) for c in core_num})\n",
    "print(\"IQR outlier rates (preview):\")\n",
    "print(preview.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be5ea9-0f49-4102-8969-86ddd1180d05",
   "metadata": {},
   "source": [
    "## 6. Data Splitting Strategy\n",
    "\n",
    "- Time-aware split by chronological order: 70% train, 15% validation, 15% test.  \n",
    "- Rationale: avoids look-ahead leakage and mirrors real predictive use.  \n",
    "- No shuffling; maintain the global timeline across all tickers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c1a52a3-4f0f-41c8-9d6f-7a99ad6f3a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (700, 11) (150, 11) (149, 11)\n",
      "Preview cols: ['open', 'high', 'low', 'close', 'volume', 'ret_1d', 'ret_5d', 'vol_ma5', 'close_ma5', 'close_ma20', 'Name']\n"
     ]
    }
   ],
   "source": [
    "TARGET = \"up_1d\"  # or \"close_next\" for regression\n",
    "\n",
    "feature_cols = [\n",
    "    \"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "    \"ret_1d\",\"ret_5d\",\"vol_ma5\",\"close_ma5\",\"close_ma20\",\"Name\"\n",
    "]\n",
    "feature_cols = [c for c in feature_cols if c in df_model.columns]\n",
    "\n",
    "X = df_model[feature_cols].copy()\n",
    "y = df_model[TARGET].copy()\n",
    "\n",
    "# Time-aware split on tiny set\n",
    "all_idx = X.index.sort_values().unique()\n",
    "n = len(all_idx)\n",
    "cut1 = all_idx[int(n*0.70)] if n > 0 else None\n",
    "cut2 = all_idx[int(n*0.85)] if n > 0 else None\n",
    "\n",
    "train_idx = X.index[X.index <= cut1] if cut1 is not None else X.index[:0]\n",
    "val_idx   = X.index[(X.index > cut1) & (X.index <= cut2)] if cut2 is not None else X.index[:0]\n",
    "test_idx  = X.index[X.index > cut2] if cut2 is not None else X.index[:0]\n",
    "\n",
    "# If the sample is too small and any split is empty, fallback to simple head/tail split\n",
    "if len(train_idx) == 0 or len(val_idx) == 0:\n",
    "    split1 = int(len(X)*0.7)\n",
    "    split2 = int(len(X)*0.85)\n",
    "    train_idx = X.index[:split1]\n",
    "    val_idx   = X.index[split1:split2]\n",
    "    test_idx  = X.index[split2:]\n",
    "\n",
    "X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "X_val,   y_val   = X.loc[val_idx],   y.loc[val_idx]\n",
    "X_test,  y_test  = X.loc[test_idx],  y.loc[test_idx]\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Preview cols:\", X_train.columns[:20].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0329c73-2bcd-4643-97a9-59d9f7d6eec9",
   "metadata": {},
   "source": [
    "## 7. Ensuring an Unbiased Dataset\n",
    "\n",
    "- Distribution alignment: ensure validation/test spans typical conditions; consider multiple validation windows if needed.  \n",
    "- Representation: verify a reasonable mix of tickers/sectors across splits; note any survivorship bias.  \n",
    "- Subgroup metrics: evaluate performance by ticker or sector and iterate on features to address gaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5ff62bd-3fe4-4cf5-94cb-41c066580086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tickers — Train/Val/Test: 1 1 1\n"
     ]
    }
   ],
   "source": [
    "if \"Name\" in X_train.columns:\n",
    "    print(\"Unique tickers — Train/Val/Test:\",\n",
    "          X_train[\"Name\"].nunique(),\n",
    "          X_val[\"Name\"].nunique(),\n",
    "          X_test[\"Name\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d294f55-fb3f-48a8-9acd-581fa6a00517",
   "metadata": {},
   "source": [
    "## 8. Features for Training\n",
    "\n",
    "- Initial features: OHLCV, returns (ret_1d, ret_5d), rolling means (vol_ma5, close_ma5, close_ma20), and Name as a categorical feature.  \n",
    "- Exclusions: any feature that leaks future information or redundant/unstable engineered fields.  \n",
    "- Future enhancements: volatility proxies, sector info, and macro covariates if justified.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61452737-63cc-481a-9f2c-2f983c95b4fd",
   "metadata": {},
   "source": [
    "## 9. Data Types\n",
    "\n",
    "- Numerical: continuous OHLCV, returns, rolling stats.  \n",
    "- Categorical: Name (ticker), optionally sector.  \n",
    "- Datetime: used for chronological splitting and possible seasonal/cyclical features later.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53cf93d1-bc9e-4cd6-a2b5-3073e5340d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cb2d9-c851-4e8e-9f6a-fbd66a0e379e",
   "metadata": {},
   "source": [
    "## 10. Transformations and Pipeline\n",
    "\n",
    "- Numeric: median imputation and standardization.  \n",
    "- Categorical: most-frequent imputation and one-hot encoding for Name with handle_unknown=\"ignore\".  \n",
    "- Implemented with scikit-learn ColumnTransformer + Pipeline to ensure reproducibility and avoid leakage.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28c324a1-0892-4a9a-84d1-0db3d284bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: ['open', 'high', 'low', 'close', 'volume', 'ret_1d', 'ret_5d', 'vol_ma5', 'close_ma5', 'close_ma20']\n",
      "Categorical cols: ['Name']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_cols = [\n",
    "    \"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "    \"ret_1d\",\"ret_5d\",\"vol_ma5\",\"close_ma5\",\"close_ma20\"\n",
    "]\n",
    "num_cols = [c for c in num_cols if c in X_train.columns]\n",
    "cat_cols = [\"Name\"] if \"Name\" in X_train.columns else []\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                          (\"scaler\", StandardScaler())]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "print(\"Numeric cols:\", num_cols); print(\"Categorical cols:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4ccc246-a6b1-493a-8cdb-53847a378644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: ['open', 'high', 'low', 'close', 'volume', 'ret_1d', 'ret_5d', 'vol_ma5', 'close_ma5', 'close_ma20']\n",
      "Categorical cols: ['Name']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_cols = [\n",
    "    \"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "    \"ret_1d\",\"ret_5d\",\"vol_ma5\",\"close_ma5\",\"close_ma20\"\n",
    "]\n",
    "num_cols = [c for c in num_cols if c in X_train.columns]\n",
    "cat_cols = [\"Name\"] if \"Name\" in X_train.columns else []\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                          (\"scaler\", StandardScaler())]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "print(\"Numeric cols:\", num_cols); print(\"Categorical cols:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d9444-d804-4c63-a362-745546d1c472",
   "metadata": {},
   "source": [
    "## 11. Baseline Models and Metrics\n",
    "\n",
    "- Classification baseline: Logistic Regression with class_weight='balanced' (TARGET = up_1d).  \n",
    "- Regression baseline: Ridge Regression (TARGET = close_next).  \n",
    "- Evaluate on validation; keep test untouched for final estimate.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6efc1b7-47df-4051-911c-12402eed2413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.67      0.57        70\n",
      "         1.0       0.58      0.40      0.47        80\n",
      "\n",
      "    accuracy                           0.53       150\n",
      "   macro avg       0.54      0.54      0.52       150\n",
      "weighted avg       0.54      0.53      0.52       150\n",
      "\n",
      "Validation ROC-AUC: 0.5183928571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([(\"prep\", preprocess),\n",
    "                (\"mdl\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))])\n",
    "clf.fit(X_train, y_train)\n",
    "val_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "try:\n",
    "    val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "    print(\"Validation ROC-AUC:\", roc_auc_score(y_val, val_proba))\n",
    "except Exception as e:\n",
    "    print(\"ROC-AUC not available:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533f1d0-e701-405f-97b6-0dc66e80ba61",
   "metadata": {},
   "source": [
    "## 12. Save Processed Data\n",
    "\n",
    "- Save a model-ready snapshot of the engineered dataset to Parquet for efficient storage and reloads.  \n",
    "- Store artifacts under data/processed with clear naming for reproducibility.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81dccca9-3366-4051-b41a-ffb3de177806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (22.0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow scikit-learn pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c21687b1-62de-4d5c-bc34-463b6f3a9820",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowKeyError",
     "evalue": "A type extension with name pandas.period already defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m PROC_DIR.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m out_path = PROC_DIR / \u001b[33m\"\u001b[39m\u001b[33msp500_all_stocks_5yr_model.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved:\u001b[39m\u001b[33m\"\u001b[39m, out_path.resolve())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages/pandas/core/frame.py:3124\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages/pandas/io/parquet.py:478\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    477\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    482\u001b[39m impl.write(\n\u001b[32m    483\u001b[39m     df,\n\u001b[32m    484\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     **kwargs,\n\u001b[32m    491\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages/pandas/io/parquet.py:64\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages/pandas/io/parquet.py:170\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages/pandas/core/arrays/arrow/extension_types.py:59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# register the type with a dummy instance\u001b[39;00m\n\u001b[32m     58\u001b[39m _period_type = ArrowPeriodType(\u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_period_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mArrowIntervalType\u001b[39;00m(pyarrow.ExtensionType):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, subtype, closed: IntervalClosedType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m# attributes need to be set first before calling\u001b[39;00m\n\u001b[32m     65\u001b[39m         \u001b[38;5;66;03m# super init (as that calls serialize)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages/pyarrow/types.pxi:2226\u001b[39m, in \u001b[36mpyarrow.lib.register_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.10/libexec/lib/python3.14/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: A type extension with name pandas.period already defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PROC_DIR = Path(\"data/processed\")\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_path = PROC_DIR / \"sp500_all_stocks_5yr_model.parquet\"\n",
    "df_model.to_parquet(out_path)\n",
    "print(\"Saved:\", out_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d5dea-ee3f-4d0f-8e74-edd84f6576fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e15c6-6645-472b-996d-06c669622d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cd755c6-8817-4053-a269-ebd8a6903a2b",
   "metadata": {},
   "source": [
    "## Submission Summary\n",
    "\n",
    "- Loaded a local S&P 500 OHLCV dataset, parsed dates, and indexed chronologically to support time-series workflows.  \n",
    "- Engineered next-day targets (close_next for regression, up_1d for classification) and core numeric features (returns and rolling means) per ticker without leakage.  \n",
    "- Applied a chronological 70/15/15 train/validation/test split to avoid look-ahead bias and mirror real predictive use.  \n",
    "- Built a reproducible scikit-learn Pipeline that imputes/scales numeric features and one-hot encodes the ticker (Name) only within the Pipeline.  \n",
    "- Trained baseline models and evaluated on the validation set, with the test set reserved for final estimation; saved a model-ready Parquet snapshot for reproducibility.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
