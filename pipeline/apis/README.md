# Data Collection - APIs  
**Holberton School Machine Learning Track**

This project focuses on collecting, processing, and transforming data from external APIs.  
Data ingestion is a fundamental component in building data pipelines and ML systems.  
Here we use multiple APIs (SWAPI, GitHub API, SpaceX API) to practice pagination, rate limits, JSON handling, and data manipulation.

---

## üìå Project Objectives

By the end of this project, you should be able to:

- Use the Python `requests` package  
- Perform HTTP GET requests  
- Handle API rate limits  
- Manage pagination in API endpoints  
- Fetch and parse JSON data  
- Extract and transform information from external services  
- Structure Python modules with full documentation  
- Respect pycodestyle (PEP8) for clean and professional code  

---

## üõ†Ô∏è Requirements

- **Python 3.9** (Ubuntu 20.04 LTS)
- Editors allowed: `vi`, `vim`, `emacs`
- All files must:
  - Start with `#!/usr/bin/env python3`
  - End with a new line
  - Be executable
  - Follow **pycodestyle 2.11.1**
- All modules, functions, and classes must have **full documentation**
- `README.md` at the project root is mandatory (this one)

---

## üìÇ Project Structure

